import os
import torch
import numpy as np
import cv2
import open3d as o3d
import torch.nn.functional as F
from PIL import Image
import argparse
from multiprocessing import Process, Queue
import yaml

from mde.utils import build_dinov2_depther
from mde.io import load_model_setup, read_depth
from mde.ops import make_depth_transform
from odometry.dpvo.utils import Timer
from odometry.dpvo.dpvo import DPVO
from odometry.dpvo.config import cfg as dpvo_cfg
from odometry.dpvo.stream import image_stream, video_stream, realsense_stream, camera_stream
from odometry.dpvo.plot_utils import plot_trajectory, save_trajectory_tum_format
from cameras.realsense import init_rs_camera

from pcd import get_pcd_geometry_with_camera_poses, build_pcd_from_rgbd
from visualizer import non_blocking_visualizer, pcdQueue

# NOTE: In case --odometry is given to this script, it runs a DPVO instance and optimizes for poses and sparse patch depths.
# For running, it uses the rgb frames in the same directory as rgb_path.

parser = argparse.ArgumentParser(prog='Visualize point cloud generated by depth estimation model.')
parser.add_argument('--rgb_path', type=str, required=True)
parser.add_argument('--odometry', action='store_true')
parser.add_argument('--pose_path', type=str)
parser.add_argument('--use_gt_depth', action='store_true')

parser.set_defaults(odometry=False)
parser.set_defaults(use_gt_depth=False)

args = parser.parse_args()

# Load yaml config file
with open("src/yarp-app/configs/default.yaml", "r") as f:
    cfg = yaml.safe_load(f)

@torch.no_grad()
def dpvo_wrapper(cfg, network, imagedir, calib, stride, skip, camera_stream, viz, timeit, output_name):
    slam = None
    queue = Queue(maxsize=8)
    # TODO: camera_stream option is not really already implemented as everything should work at real time.
    if camera_stream:
        # width = 640
        # height = 480
        # (
        #     pipeline, 
        #     align, 
        #     _, 
        #     _, 
        #     depth_scale_meters, 
        #     _, 
        #     _ 
        # ) = init_rs_camera(width, height)
        camera_idx = 6
        reader = Process(target=camera_stream, args=(queue, camera_idx, calib, stride, skip))
    else:
        if os.path.isdir(imagedir):
            reader = Process(target=image_stream, args=(queue, imagedir, calib, stride, skip))
        else:
            reader = Process(target=video_stream, args=(queue, imagedir, calib, stride, skip))

    reader.start()

    print("Optimizing poses and sparse patch depths...")
    while 1:
        (t, image, intrinsics) = queue.get()
        if t < 0: break

        image = torch.from_numpy(image).permute(2,0,1).cuda()
        intrinsics = torch.from_numpy(intrinsics).cuda()

        if slam is None:
            slam = DPVO(cfg, network, ht=image.shape[1], wd=image.shape[2], viz=viz)

        image = image.cuda()
        intrinsics = intrinsics.cuda()

        with Timer("SLAM", enabled=timeit):
            print("DEBUG: Entering update step")
            slam(t, image, intrinsics)

    for _ in range(12):
        slam.update()

    reader.join()

    print("Dumping poses and patch depths...")
    poses, _ = slam.dump_poses(output_name)
    x, y, inv_depths = slam.dump_patch_depths(output_name)
    slam.terminate()

    return poses, x, y, inv_depths

def main():
    # Camera intrinsics matrix definition
    intr_matr = np.zeros((3,3))
    intr_matr[0, 0] = 614.13299560546875
    intr_matr[1, 1] = 614.47918701171875
    intr_matr[0, 2] = 318.909332275390625
    intr_matr[1, 2] = 249.5451202392578125

    if args.odometry:
        imagedir = '/'.join(args.rgb_path.split('/')[:-1])
        print(f'Retrieving images from path {imagedir}')

        calib = cfg['global']['camera_intrinsics']
        stride = cfg['odometry']['dpvo_config']['STRIDE']
        skip = cfg['odometry']['dpvo_config']['SKIP_FRAMES']
        buffer = cfg['odometry']['dpvo_config']['BUFFER_SIZE']
        network = os.path.join(cfg['odometry']['model_ckpts_path'], f"{cfg['odometry']['model_name']}.pth")

        opts = []
        for key, value in cfg['odometry']['dpvo_config'].items():
            if key in dpvo_cfg:
                opts.append(key)
                opts.append(value)

        dpvo_cfg.merge_from_list(opts)
        dpvo_cfg.BUFFER_SIZE = buffer

        dpvo_poses, x, y, dpvo_inv_depths = dpvo_wrapper(
            cfg=dpvo_cfg,
            network=network,
            imagedir=imagedir,
            calib=calib,
            stride=stride,
            skip=skip,
            camera_stream=False,
            viz=False,
            timeit=True,
            output_name='tmp' # Saves odometry poses and depths in tmp.npy (results/dpvo/[depths/poses]/tmp.npy).
        )

    else: 
        # Check if args.pose_path is given by the user
        assert args.pose_path
        pose_path = args.pose_path
        patch_path = pose_path.replace('poses', 'depths')

        dpvo_poses = np.load(pose_path)

        # Load x,y of tracked patches and corresponding depths for the first frame.
        x, y, dpvo_inv_depths = np.load(patch_path)

    if not args.use_gt_depth:
        # Load model config from model name (model configs are indexed from model name)
        model_cfg = load_model_setup(cfg) 
        model = build_dinov2_depther(cfg,
                                backbone_size='small', 
                                head_type=model_cfg['head'], 
                                min_depth=0, 
                                max_depth=2,
                                inference=True,
                                checkpoint_name=cfg['mde']['model_name'],
                                use_depth_anything_encoder=True if cfg['mde']['backbone'] == 'depth-anything' else False)
        model.eval()
        model.cuda()
        
        # Load RGB Input and apply transform to it
        transform = make_depth_transform(
            backbone=cfg['mde']['backbone']
        )
        rgb = transform(Image.open(args.rgb_path))
        
        # Run inference on RGB input
        with torch.inference_mode():
            rgb = rgb.unsqueeze(0)
            depth = model.whole_inference(rgb.cuda(), img_meta=None, rescale=False)
            # Downscale to 1/4 resolution with average pooling
            avg_pooled_depth = F.avg_pool2d(depth, kernel_size=4, stride=4).squeeze().cpu()
            depth = depth.squeeze().cpu().numpy()
            
    else:
        #NOTE: We always assume that depth path is equal to rgb_path.replace('rgb', 'depth')
        depth_path = args.rgb_path.replace('rgb', 'depth').replace('jpg', 'float')
        depth = read_depth(depth_path)
        avg_pooled_depth = F.avg_pool2d(torch.tensor(depth).reshape(1, 1, depth.shape[0], depth.shape[1]), kernel_size=4, stride=4).squeeze().cpu()

    depth = (depth * 1000).astype(np.uint16)
    # Use avg_pooled_pred to estimate scale factor and offset to align the estimated absolute depth with the relative depth from DPVO
    # see https://github.com/princeton-vl/DPVO/issues/44
    valid_mask = np.bitwise_and((1/dpvo_inv_depths) >= 0, (1/dpvo_inv_depths) <=2)
    
    print(1/dpvo_inv_depths)
    print(avg_pooled_depth[y, x][valid_mask])
    print(dpvo_inv_depths[valid_mask])

    scale_factor = np.median(avg_pooled_depth[y, x][valid_mask] * dpvo_inv_depths[valid_mask])

    # Retrieve camera bgr
    camera_bgr = cv2.imread(args.rgb_path)

    geometries = get_pcd_geometry_with_camera_poses(camera_bgr, depth, intr_matr, dpvo_poses, scale_factor)
    o3d.visualization.draw_geometries(geometries)


if __name__ == '__main__':
    torch.cuda.empty_cache()
    main()


